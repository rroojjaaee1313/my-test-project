import streamlit as st
import google.generativeai as genai
from PyPDF2 import PdfReader
from gtts import gTTS
import os
import io
import PIL.Image
import json

# --- 1. ç³»çµ±åˆå§‹åŒ– (å¼·åŒ–æ¨¡å‹ç›¸å®¹æ€§) ---
st.set_page_config(page_title="è€é·¹ AI é•·æœŸåŠ©ç†", layout="wide")

try:
    if "GEMINI_API_KEY" in st.secrets:
        API_KEY = st.secrets["GEMINI_API_KEY"]
        genai.configure(api_key=API_KEY)
    else:
        st.error("âŒ æ‰¾ä¸åˆ° API é‡‘é‘°ï¼Œè«‹æª¢æŸ¥ Streamlit Secrets è¨­å®šã€‚")
        st.stop()

    # è‡ªå‹•åµæ¸¬å¯ç”¨çš„æ¨¡å‹åç¨± (é¿é–‹ v1beta 404 å•é¡Œ)
    available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
    # å„ªå…ˆæ‰¾ 1.5 flashï¼Œå¦‚æœæ²’æœ‰å°±æ‰¾ç¬¬ä¸€å€‹å¯ç”¨çš„
    MODEL_NAME = 'models/gemini-1.5-flash' if 'models/gemini-1.5-flash' in available_models else available_models[0]
    
    model = genai.GenerativeModel(model_name=MODEL_NAME)
    
except Exception as e:
    st.error(f"âŒ ç³»çµ±å•Ÿå‹•å¤±æ•—ï¼Œè«‹ç¢ºèª API Key æ˜¯å¦æ­£ç¢ºã€‚éŒ¯èª¤è¨Šæ¯: {e}")
    st.stop()

# --- 2. è¨˜æ†¶åŠŸèƒ½ ---
HISTORY_FILE = "chat_history.json"

def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except: return []
    return []

def save_history(history):
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

if "messages" not in st.session_state:
    st.session_state.messages = load_history()

# --- 3. ä»‹é¢è¨­è¨ˆ ---
st.title("ğŸ¦… è€é·¹åœ˜éšŠï¼šé•·æœŸ AI æ™ºæ…§åŠ©ç†")
st.caption(f"ç›®å‰é‹ä½œæ¨¡å‹: {MODEL_NAME}") # æ–¹ä¾¿æˆ‘å€‘æª¢æŸ¥

with st.sidebar:
    st.header("âš™ï¸ åŠ©ç†ç®¡ç†")
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰å°è©±ç´€éŒ„"):
        st.session_state.messages = []
        save_history([])
        st.rerun()
    st.divider()
    uploaded_pdf = st.file_uploader("ä¸Šå‚³åŸ¹è¨“æ•™æ (PDF)", type="pdf")
    uploaded_image = st.file_uploader("ä¸Šå‚³å°è©±æˆªåœ– (åˆ†æç”¨)", type=["png", "jpg", "jpeg"])

# PDF è™•ç†é‚è¼¯
context_text = ""
if uploaded_pdf:
    reader = PdfReader(uploaded_pdf)
    for page in reader.pages:
        context_text += page.extract_text() + "\n"
    st.sidebar.success("âœ… æ•™æå·²è¼‰å…¥")

# é¡¯ç¤ºæ­·å²ç´€éŒ„
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- 4. å•ç­”é‚è¼¯ ---
if prompt := st.chat_input("è«‹å•å°å¸«..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    
    content_list = []
    if uploaded_image:
        img = PIL.Image.open(uploaded_image)
        content_list.append(img)

    history_context = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages[-3:]])
    full_prompt = f"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è€é·¹åœ˜éšŠå°å¸«ã€‚æ•™æå…§å®¹ï¼š\n{context_text[:4000]}\nè¿‘æœŸå°è©±ï¼š\n{history_context}\nç¾åœ¨å•é¡Œï¼š{prompt}"
    content_list.append(full_prompt)

    with st.chat_message("assistant"):
        with st.spinner("è€é·¹å°å¸«æ€è€ƒä¸­..."):
            try:
                response = model.generate_content(content_list)
                full_response = response.text
                st.markdown(full_response)
                
                # èªéŸ³ç”Ÿæˆ
                tts = gTTS(text=full_response[:100], lang='zh-tw')
                audio_fp = io.BytesIO()
                tts.write_to_fp(audio_fp)
                st.audio(audio_fp, format='audio/mp3')

                # å„²å­˜ç´€éŒ„
                st.session_state.messages.append({"role": "user", "content": prompt})
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                save_history(st.session_state.messages)
            except Exception as e:
                st.error(f"âš ï¸ å‘¼å« AI å¤±æ•—: {e}")
